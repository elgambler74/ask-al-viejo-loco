import streamlit as st
import openai
import json
import numpy as np
import pathlib
import tempfile
import streamlit.components.v1 as components
import os
import pickle
from sklearn.neighbors import NearestNeighbors
from sklearn.metrics.pairwise import cosine_similarity

from moviepy.editor import VideoFileClip
import imageio_ffmpeg

# OpenAI client setup (new syntax)
client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Paths
EMBEDDINGS_FILE = "embeddings.pkl"
METADATA_FILE = "chunk_metadata.json"
VIDEO_FOLDER = "."

# Load embeddings and metadata
@st.cache_data
def load_search_data():
    try:
        # Try to load existing embeddings
        with open(EMBEDDINGS_FILE, "rb") as f:
            embeddings = pickle.load(f)
    except FileNotFoundError:
        st.error(f"‚ùå Embeddings file not found: {EMBEDDINGS_FILE}")
        st.info("You need to create embeddings from your FAISS index first.")
        return None, None, None
    
    try:
        with open(METADATA_FILE, "r", encoding="utf-8") as f:
            metadata = json.load(f)
    except FileNotFoundError:
        st.error(f"‚ùå Metadata file not found: {METADATA_FILE}")
        return None, None, None
    
    # Create NearestNeighbors model
    nn_model = NearestNeighbors(n_neighbors=10, metric='cosine')
    nn_model.fit(embeddings)
    
    return embeddings, metadata, nn_model

# Parse SRT timecode to seconds
def parse_srt_timecode(ts: str):
    h, m, s = ts.replace(",", ".").split(":")
    return float(h) * 3600 + float(m) * 60 + float(s)

# Clip video dynamically using moviepy
def clip_video_segment(filename, start_sec, duration):
    clip_path = pathlib.Path(filename)
    if not clip_path.exists():
        print(f"‚ùå File not found: {clip_path}")
        return None
    try:
        clip = VideoFileClip(str(clip_path)).subclip(start_sec, start_sec + duration)
        tmpfile = tempfile.NamedTemporaryFile(suffix=".mp4", delete=False)
        clip.write_videofile(tmpfile.name, codec="libx264", audio_codec="aac", verbose=False, logger=None)
        return tmpfile.name
    except Exception as e:
        print(f"‚ùå Error generating clip: {e}")
        return None

# Embedding lookup (updated syntax)
def embed_query(query: str) -> np.ndarray:
    res = client.embeddings.create(
        model="text-embedding-3-large",
        input=query
    )
    return np.array(res.data[0].embedding, dtype=np.float32)

# Semantic search using scikit-learn
def search(query, embeddings, metadata, nn_model, k=3):
    query_embedding = embed_query(query).reshape(1, -1)
    distances, indices = nn_model.kneighbors(query_embedding, n_neighbors=k)
    
    results = []
    for i, idx in enumerate(indices[0]):
        result = metadata[idx].copy()
        result['similarity'] = 1 - distances[0][i]  # Convert distance to similarity
        results.append(result)
    
    return results

# GPT summary (updated syntax)
@st.cache_data(show_spinner=False)
def summarize_with_gpt(text):
    try:
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "Summarize this Holocaust survivor testimony excerpt in 1‚Äì2 sentences."},
                {"role": "user", "content": text}
            ]
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"‚ö†Ô∏è Summary failed: {e}"

# Migration function to convert FAISS data to scikit-learn format
def convert_faiss_to_sklearn():
    st.info("üîÑ Converting FAISS data to scikit-learn format...")
    
    try:
        import faiss
        # Load FAISS index
        index = faiss.read_index("faiss_index.index")
        
        # Extract all embeddings
        embeddings = index.reconstruct_n(0, index.ntotal)
        
        # Save as pickle file
        with open(EMBEDDINGS_FILE, "wb") as f:
            pickle.dump(embeddings, f)
        
        st.success("‚úÖ Successfully converted FAISS data to scikit-learn format!")
        return True
    except Exception as e:
        st.error(f"‚ùå Failed to convert FAISS data: {e}")
        return False

# Main UI
st.title("üé• Ask al Viejo Loco")

# Load search data
embeddings, metadata, nn_model = load_search_data()

# If data couldn't be loaded, try to convert from FAISS
if embeddings is None:
    st.warning("‚ö†Ô∏è Search data not found. Attempting to convert from FAISS...")
    if st.button("Convert FAISS to scikit-learn"):
        if convert_faiss_to_sklearn():
            st.rerun()
    st.stop()

# Search interface
query = st.text_input("Ask a question:", placeholder="e.g. ¬øD√≥nde menciona el gueto de Varsovia?")
clip_duration = st.slider("Clip length (seconds):", min_value=10, max_value=120, value=60, step=10)

if query:
    st.success("Searching...")

    try:
        results = search(query, embeddings, metadata, nn_model, k=3)
    except Exception as e:
        st.error(f"‚ùå Search error: {e}")
        results = []

    st.markdown("## üéØ Top Matches")

    for i, result in enumerate(results, 1):
        st.markdown(f"### Match #{i}")
        st.markdown(f"**Timestamp:** {result['start']} ‚Üí {result['end']}")
        st.markdown(f"**Video File:** `{result['video']}`")
        st.markdown(f"**Similarity:** {result.get('similarity', 0):.3f}")

        with st.spinner("Summarizing..."):
            summary = summarize_with_gpt(result["text"])
        st.markdown(f"**üß† Summary:** {summary}")

        # Clip and play
        start_sec = parse_srt_timecode(result['start'])
        video_file = result['video']  # expects `shrunk_*.mp4` to be present
        video_path = pathlib.Path(VIDEO_FOLDER) / video_file

        if video_path.exists():
            clipped = clip_video_segment(video_path, start_sec, clip_duration)
            if clipped:
                st.video(clipped)
            else:
                st.warning("‚ö†Ô∏è Could not generate video clip.")
        else:
            st.warning(f"‚ö†Ô∏è Video not found: {video_file}")

        # Transcript
        video_stem = result['video'].replace(".mp4", "")
        srt_path = pathlib.Path("transcripts") / f"{video_stem}.srt"

        if srt_path.exists():
            with srt_path.open("r", encoding="utf-8") as f:
                blocks = f.read().strip().split("

")

            html_transcript = ""
            for block in blocks:
                lines = block.strip().splitlines()
                if len(lines) < 3:
                    continue
                start, end = lines[1].split(" --> ")
                text = " ".join(lines[2:])
                is_match = (start == result['start'] and end == result['end'])
                if is_match:
                    html_transcript += f"<div style='background-color:#ffe0e0; padding:6px; margin-bottom:6px; border-left:4px solid #e91e63'><b>{start} ‚Üí {end}</b><br><b>{text}</b></div>"
                else:
                    html_transcript += f"<div style='padding:4px; margin-bottom:4px;'><i>{start} ‚Üí {end}</i><br>{text}</div>"

            st.markdown("#### üìú Full Transcript")
            components.html(
                f"""
                <div style='max-height: 300px; overflow-y: auto; background-color: #fdfdfd; padding: 10px; border: 1px solid #ccc; border-radius: 6px; font-size: 14px; line-height: 1.4'>
                    {html_transcript}
                </div>
                """,
                height=320,
                scrolling=True
            )
        else:
            st.info("Transcript not found.")